// Lexer module - tokenizer using logos

use crate::span::{PositionTracker, Span};
use logos::Logos;

#[derive(Logos, Debug, PartialEq, Clone)]
#[logos(skip r"[ \t\r\n\f]+")] // Skip whitespace
#[logos(skip r"#.*")] // Skip comments
pub enum Token {
    // Doc comments must come before String to match first
    // Doc comments (Python-style triple quotes)
    // Matches """...""" including multiline
    // Using a custom regex that handles newlines
    #[regex(r#""""[^"]*""""#, |lex| {
        let s = lex.slice();
        // Remove triple quotes (first 3 and last 3 chars)
        if s.len() >= 6 {
            let content = &s[3..s.len()-3];
            // Trim leading/trailing whitespace but preserve internal formatting
            content.trim().to_string()
        } else {
            String::new()
        }
    })]
    DocComment(String),
    // Keywords
    #[token("fn")]
    Fn,
    #[token("let")]
    Let,
    #[token("var")]
    Var,
    #[token("return")]
    Return,
    #[token("if")]
    If,
    #[token("else")]
    Else,
    #[token("for")]
    For,
    #[token("in")]
    In,
    #[token("while")]
    While,
    #[token("break")]
    Break,
    #[token("continue")]
    Continue,
    #[token("class")]
    Class,
    #[token("new")]
    New,
    #[token("import")]
    Import,
    #[token("async")]
    Async,
    #[token("await")]
    Await,
    #[token("true")]
    True,
    #[token("false")]
    False,
    #[token("None")]
    None,
    #[token("isinstance")]
    IsInstance,
    #[token("dynamic")]
    Dynamic,

    // Type keywords
    #[token("int")]
    Int,
    #[token("str")]
    Str,
    #[token("float32")]
    Float32,
    #[token("float64")]
    Float64,
    #[token("bool")]
    Bool,
    #[token("list")]
    List,
    #[token("array")]
    Array,
    #[token("map")]
    Map,
    #[token("Tensor")]
    Tensor,

    // Operators
    #[token("+")]
    Plus,
    #[token("-")]
    Minus,
    #[token("*")]
    Star,
    #[token("/")]
    Slash,
    #[token("%")]
    Percent,
    #[token("==")]
    EqEq,
    #[token("!=")]
    Ne,
    #[token("<")]
    Lt,
    #[token(">")]
    Gt,
    #[token("<=")]
    Le,
    #[token(">=")]
    Ge,
    #[token("=")]
    Assign,
    #[token("+=")]
    PlusEq,
    #[token("-=")]
    MinusEq,
    #[token("*=")]
    StarEq,
    #[token("/=")]
    SlashEq,
    #[token("->")]
    Arrow,
    #[token("&&")]
    And,
    #[token("||")]
    Or,
    #[token("!")]
    Not,

    // Punctuation
    #[token("(")]
    LParen,
    #[token(")")]
    RParen,
    #[token("[")]
    LBracket,
    #[token("]")]
    RBracket,
    #[token("{")]
    LBrace,
    #[token("}")]
    RBrace,
    #[token(",")]
    Comma,
    #[token(":")]
    Colon,
    #[token(";")]
    Semicolon,
    #[token(".")]
    Dot,
    #[token("@")]
    At,

    // Literals
    #[regex(r"[0-9]+", |lex| lex.slice().parse().ok())]
    Integer(i64),

    #[regex(r"[0-9]+\.[0-9]+([eE][+-]?[0-9]+)?[fF]?", |lex| {
        let s = lex.slice();
        s.trim_end_matches(['f', 'F']).parse().ok()
    })]
    Float(f64),

    #[regex(r#""([^"\\]|\\.)*""#, |lex| {
        let s = lex.slice();
        // Remove quotes and unescape
        s[1..s.len()-1].to_string()
    })]
    String(String),

    #[regex(r#"f"([^"\\]|\\.)*""#, |lex| {
        let s = lex.slice();
        // Remove f" prefix and closing quote
        s[2..s.len()-1].to_string()
    })]
    FString(String),

    // Identifiers
    #[regex(r"[a-zA-Z_][a-zA-Z0-9_]*", |lex| lex.slice().to_string())]
    Ident(String),

    // Indentation tokens (generated by wrapper, not by logos directly)
    Indent,
    Dedent,
    Newline, // Used internally for indentation tracking
}

/// Token with source location information
#[derive(Debug, Clone)]
pub struct TokenWithSpan {
    pub token: Token,
    pub span: Span,
}

/// Process source code and insert Indent/Dedent tokens based on indentation
/// Returns a vector of tokens with Indent/Dedent inserted appropriately, with span information
pub fn tokenize_with_indentation(source: &str) -> Vec<Result<TokenWithSpan, ()>> {
    let tracker = PositionTracker::new(source);
    let lines: Vec<&str> = source.lines().collect();
    let mut tokens = Vec::new();
    let mut indent_stack = vec![0];
    let mut byte_offset = 0;

    for (_line_num, line) in lines.iter().enumerate() {
        let line_start_offset = byte_offset;
        let line_indent_start = byte_offset;

        // Skip empty lines
        if line.trim().is_empty() {
            byte_offset += line.len() + 1; // +1 for newline
            continue;
        }

        // Count leading spaces/tabs (treat tab as 4 spaces)
        let indent = line
            .chars()
            .take_while(|c| *c == ' ' || *c == '\t')
            .map(|c| if c == '\t' { 4 } else { 1 })
            .sum();

        let current_level = *indent_stack.last().unwrap();

        // Handle indentation changes
        if indent > current_level {
            // Indent: increase indentation
            indent_stack.push(indent);
            let indent_pos = tracker.position_at(line_indent_start);
            tokens.push(Ok(TokenWithSpan {
                token: Token::Indent,
                span: Span::single(indent_pos),
            }));
        } else if indent < current_level {
            // Dedent: decrease indentation
            while let Some(&level) = indent_stack.last() {
                if level > indent {
                    indent_stack.pop();
                    let dedent_pos = tracker.position_at(line_indent_start);
                    tokens.push(Ok(TokenWithSpan {
                        token: Token::Dedent,
                        span: Span::single(dedent_pos),
                    }));
                } else {
                    break;
                }
            }

            // Check for indentation error
            if let Some(&level) = indent_stack.last() {
                if level != indent {
                    // Indentation error - but continue for now
                }
            }
        }

        // Tokenize the line content
        let trimmed = line.trim_start();
        let trim_offset = line.len() - trimmed.len();
        if trimmed.is_empty() || trimmed.starts_with('#') {
            byte_offset += line.len() + 1;
            continue;
        }

        let mut line_lexer = Token::lexer(trimmed);
        while let Some(token_result) = line_lexer.next() {
            match token_result {
                Ok(token) => {
                    let token_span = line_lexer.span();
                    let start_offset = line_start_offset + trim_offset + token_span.start;
                    let end_offset = line_start_offset + trim_offset + token_span.end;
                    let start_pos = tracker.position_at(start_offset);
                    let end_pos = tracker.position_at(end_offset);
                    tokens.push(Ok(TokenWithSpan {
                        token,
                        span: Span::new(start_pos, end_pos),
                    }));
                }
                Err(e) => tokens.push(Err(e)),
            }
        }

        byte_offset += line.len() + 1; // +1 for newline
    }

    // Emit final dedents at EOF
    while indent_stack.len() > 1 {
        indent_stack.pop();
        let eof_pos = tracker.position_at(byte_offset);
        tokens.push(Ok(TokenWithSpan {
            token: Token::Dedent,
            span: Span::single(eof_pos),
        }));
    }

    tokens
}

/// Lexer wrapper that handles indentation
pub struct IndentLexer {
    tokens: Vec<Result<TokenWithSpan, ()>>,
    pos: usize,
}

impl IndentLexer {
    pub fn new(source: &str) -> Self {
        let tokens = tokenize_with_indentation(source);
        Self { tokens, pos: 0 }
    }
}

impl Iterator for IndentLexer {
    type Item = Result<TokenWithSpan, ()>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.pos < self.tokens.len() {
            let token = self.tokens[self.pos].clone();
            self.pos += 1;
            Some(token)
        } else {
            None
        }
    }
}

// Keep the old type alias for backward compatibility, but mark as deprecated
#[deprecated(note = "Use IndentLexer for proper indentation support")]
pub type Lexer<'source> = logos::Lexer<'source, Token>;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_keywords() {
        let mut lexer = Token::lexer("fn let var return if else");
        assert_eq!(lexer.next(), Some(Ok(Token::Fn)));
        assert_eq!(lexer.next(), Some(Ok(Token::Let)));
        assert_eq!(lexer.next(), Some(Ok(Token::Var)));
        assert_eq!(lexer.next(), Some(Ok(Token::Return)));
        assert_eq!(lexer.next(), Some(Ok(Token::If)));
        assert_eq!(lexer.next(), Some(Ok(Token::Else)));
    }

    #[test]
    fn test_operators() {
        let mut lexer = Token::lexer("+ - * / == != < >");
        assert_eq!(lexer.next(), Some(Ok(Token::Plus)));
        assert_eq!(lexer.next(), Some(Ok(Token::Minus)));
        assert_eq!(lexer.next(), Some(Ok(Token::Star)));
        assert_eq!(lexer.next(), Some(Ok(Token::Slash)));
        assert_eq!(lexer.next(), Some(Ok(Token::EqEq)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ne)));
        assert_eq!(lexer.next(), Some(Ok(Token::Lt)));
        assert_eq!(lexer.next(), Some(Ok(Token::Gt)));
    }

    #[test]
    fn test_literals() {
        let mut lexer = Token::lexer("123 45.67 \"hello\" f\"world\"");
        assert_eq!(lexer.next(), Some(Ok(Token::Integer(123))));
        assert_eq!(lexer.next(), Some(Ok(Token::Float(45.67))));
        assert_eq!(lexer.next(), Some(Ok(Token::String("hello".to_string()))));
        assert_eq!(lexer.next(), Some(Ok(Token::FString("world".to_string()))));
    }

    #[test]
    fn test_identifiers() {
        let mut lexer = Token::lexer("foo bar_123 xyz");
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("foo".to_string()))));
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("bar_123".to_string()))));
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("xyz".to_string()))));
    }

    #[test]
    fn test_function_declaration() {
        let mut lexer = Token::lexer("fn greet(name: str) -> str:");
        assert_eq!(lexer.next(), Some(Ok(Token::Fn)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("greet".to_string()))));
        assert_eq!(lexer.next(), Some(Ok(Token::LParen)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("name".to_string()))));
        assert_eq!(lexer.next(), Some(Ok(Token::Colon)));
        assert_eq!(lexer.next(), Some(Ok(Token::Str)));
        assert_eq!(lexer.next(), Some(Ok(Token::RParen)));
        assert_eq!(lexer.next(), Some(Ok(Token::Arrow)));
        assert_eq!(lexer.next(), Some(Ok(Token::Str)));
        assert_eq!(lexer.next(), Some(Ok(Token::Colon)));
    }

    #[test]
    fn test_comments() {
        let mut lexer = Token::lexer("fn # comment\nmain");
        assert_eq!(lexer.next(), Some(Ok(Token::Fn)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ident("main".to_string()))));
    }

    #[test]
    fn test_doc_comment() {
        let mut lexer = Token::lexer(r#""""This is a doc comment."""fn"#);
        assert_eq!(
            lexer.next(),
            Some(Ok(Token::DocComment("This is a doc comment.".to_string())))
        );
        assert_eq!(lexer.next(), Some(Ok(Token::Fn)));
    }

    #[test]
    fn test_indentation_tokens() {
        let source = "fn test():\n    let x = 10\n    let y = 20";
        let tokens: Vec<_> = tokenize_with_indentation(source)
            .into_iter()
            .filter_map(|r| r.ok())
            .map(|t| t.token)
            .collect();

        assert!(tokens.contains(&Token::Indent));
        assert!(tokens.contains(&Token::Fn));
        assert!(tokens.contains(&Token::Let));
    }

    #[test]
    fn test_nested_indentation() {
        let source = "fn test():\n    if True:\n        return 1\n    return 0";
        let tokens: Vec<_> = tokenize_with_indentation(source)
            .into_iter()
            .filter_map(|r| r.ok())
            .map(|t| t.token)
            .collect();

        let indent_count = tokens.iter().filter(|t| matches!(t, Token::Indent)).count();
        assert!(
            indent_count >= 2,
            "Should have multiple indents for nested blocks"
        );
    }

    #[test]
    fn test_dedent_tokens() {
        let source = "fn test():\n    let x = 10\nreturn 0";
        let tokens: Vec<_> = tokenize_with_indentation(source)
            .into_iter()
            .filter_map(|r| r.ok())
            .map(|t| t.token)
            .collect();

        assert!(
            tokens.contains(&Token::Dedent),
            "Should have dedent when indentation decreases"
        );
    }

    #[test]
    fn test_string_escaping() {
        let mut lexer = Token::lexer(r#""hello\nworld""#);
        let token = lexer.next().unwrap().unwrap();
        if let Token::String(s) = token {
            assert!(s.contains("\\n"), "Should preserve escape sequences");
        } else {
            panic!("Expected String token");
        }
    }

    #[test]
    fn test_fstring_with_expression() {
        let mut lexer = Token::lexer(r#"f"Hello {name}""#);
        let token = lexer.next().unwrap().unwrap();
        if let Token::FString(s) = token {
            assert!(s.contains("{name}"), "F-string should contain expression");
        } else {
            panic!("Expected FString token");
        }
    }

    #[test]
    fn test_float_scientific_notation() {
        let mut lexer = Token::lexer("1.5e10 2.3E-5");
        assert_eq!(lexer.next(), Some(Ok(Token::Float(1.5e10))));
        assert_eq!(lexer.next(), Some(Ok(Token::Float(2.3e-5))));
    }

    #[test]
    fn test_comparison_operators() {
        let mut lexer = Token::lexer("<= >= == !=");
        assert_eq!(lexer.next(), Some(Ok(Token::Le)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ge)));
        assert_eq!(lexer.next(), Some(Ok(Token::EqEq)));
        assert_eq!(lexer.next(), Some(Ok(Token::Ne)));
    }

    #[test]
    fn test_assignment_operators() {
        let mut lexer = Token::lexer("+= -= *= /=");
        assert_eq!(lexer.next(), Some(Ok(Token::PlusEq)));
        assert_eq!(lexer.next(), Some(Ok(Token::MinusEq)));
        assert_eq!(lexer.next(), Some(Ok(Token::StarEq)));
        assert_eq!(lexer.next(), Some(Ok(Token::SlashEq)));
    }

    #[test]
    fn test_logical_operators() {
        let mut lexer = Token::lexer("&& || !");
        assert_eq!(lexer.next(), Some(Ok(Token::And)));
        assert_eq!(lexer.next(), Some(Ok(Token::Or)));
        assert_eq!(lexer.next(), Some(Ok(Token::Not)));
    }

    #[test]
    fn test_type_keywords() {
        let mut lexer = Token::lexer("int str float32 float64 bool list array map Tensor");
        assert_eq!(lexer.next(), Some(Ok(Token::Int)));
        assert_eq!(lexer.next(), Some(Ok(Token::Str)));
        assert_eq!(lexer.next(), Some(Ok(Token::Float32)));
        assert_eq!(lexer.next(), Some(Ok(Token::Float64)));
        assert_eq!(lexer.next(), Some(Ok(Token::Bool)));
        assert_eq!(lexer.next(), Some(Ok(Token::List)));
        assert_eq!(lexer.next(), Some(Ok(Token::Array)));
        assert_eq!(lexer.next(), Some(Ok(Token::Map)));
        assert_eq!(lexer.next(), Some(Ok(Token::Tensor)));
    }

    #[test]
    fn test_boolean_literals() {
        let mut lexer = Token::lexer("true false");
        assert_eq!(lexer.next(), Some(Ok(Token::True)));
        assert_eq!(lexer.next(), Some(Ok(Token::False)));
    }

    #[test]
    fn test_none_literal() {
        let mut lexer = Token::lexer("None");
        assert_eq!(lexer.next(), Some(Ok(Token::None)));
    }

    #[test]
    fn test_empty_string() {
        let mut lexer = Token::lexer(r#""""#);
        assert_eq!(lexer.next(), Some(Ok(Token::String("".to_string()))));
    }

    #[test]
    fn test_span_tracking() {
        let source = "fn test()";
        let tokens = tokenize_with_indentation(source);
        assert!(!tokens.is_empty());
        if let Ok(token_span) = &tokens[0] {
            assert!(token_span.span.line() > 0);
        }
    }
}
